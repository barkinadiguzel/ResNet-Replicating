# ğŸ—» ResNet50 PyTorch Implementation

This repository contains a replication of **ResNet50 (Deep Residual Learning for Image Recognition)** using PyTorch. The goal is to reproduce the **ResNet50 architecture** with bottleneck blocks for ImageNet classification.

- Only **ResNet50** has been implemented.  
- Architecture follows **Conv1 â†’ Conv2_x â†’ Conv3_x â†’ Conv4_x â†’ Conv5_x â†’ Pool â†’ Flatten â†’ FC** sequence.  
**Paper**: [Deep Residual Learning for Image Recognition (CVPR 2016)](https://arxiv.org/abs/1512.03385)

> ğŸ› ï¸ Users may need to adjust the code slightly to implement other ResNet variants (like ResNet18, ResNet34, or ResNet101) or custom architectures.
---

## ğŸ— Project Structure

```bash
ResNet50-Replicating/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ layers/
â”‚   â”‚   â”œâ”€â”€ conv1.py             # Initial 7x7 Conv + BN + ReLU
â”‚   â”‚   â”œâ”€â”€ conv2_x.py           # Conv2 bottleneck blocks
â”‚   â”‚   â”œâ”€â”€ conv3_x.py           # Conv3 bottleneck blocks
â”‚   â”‚   â”œâ”€â”€ conv4_x.py           # Conv4 bottleneck blocks
â”‚   â”‚   â”œâ”€â”€ conv5_x.py           # Conv5 bottleneck blocks
â”‚   â”‚   â”œâ”€â”€ pool_layers/
â”‚   â”‚   â”‚   â”œâ”€â”€ maxpool_layer.py # MaxPool2d after Conv1
â”‚   â”‚   â”‚   â””â”€â”€ avgpool_layer.py # Global Average Pooling after Conv5
â”‚   â”‚   â”œâ”€â”€ flatten_layer.py     # Conv â†’ FC transition
â”‚   â”‚   â””â”€â”€ fc_layer.py          # Fully Connected Layer (1000 classes)
â”‚   â”‚
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â””â”€â”€ resnet50.py          # Conv1 â†’ Conv2_x â†’ Conv3_x â†’ Conv4_x â†’ Conv5_x â†’ Pool â†’ Flatten â†’ FC
â”‚   â”‚
â”‚   â””â”€â”€ config.py                # Hyperparameters, optimizer
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ã€½ï¸ Figures

### Figure 2 â€“ Residual Connections
![Residual Connections](images/figu2.png)  
- Shows how shortcut connections bypass convolutional layers, allowing the input to be directly added to the output. This helps gradients flow smoothly in very deep networks and stabilizes training. Residual connections make it easier to learn identity mappings if needed.

### Figure 5 â€“ Bottleneck Block
![Bottleneck Block](images/figu5.png)  
- Illustrates the 1Ã—1 â†’ 3Ã—3 â†’ 1Ã—1 convolution sequence used in deeper ResNets like ResNetâ€‘50. The first 1Ã—1 reduces channels, the 3Ã—3 processes features, and the last 1Ã—1 restores channels. This design reduces parameters while keeping network depth.

### Table 1 â€“ ResNet Variants
![ResNet Variants](images/tab1.png)  
- Lists ResNet models (18, 34, 50, 101, 152) and their layer configurations. Shows how many bottleneck blocks each stage contains and where strides change. Helps compare depth and complexity across different variants.

---

## ğŸ”— Feedback

For questions or feedback, contact: [barkin.adiguzel@gmail.com](mailto:barkin.adiguzel@gmail.com)


